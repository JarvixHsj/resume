
首先，一份好的简历不光说明事实，更通过FAB模式来增强其说服力。

 - Feature：是什么
 - Advantage：比别人好在哪些地方
 - Benefit：如果雇佣你，招聘方会得到什么好处 

其次，写简历和写议论文不同，过分的论证会显得自夸，反而容易引起反感，所以要点到为止。这里的技巧是，提供论据，把论点留给阅读简历的人自己去得出。放论据要具体，最基本的是要数字化，好的论据要让人印象深刻。

举个例子，下边内容是虚构的：

2006年，我参与了手机XX网发布系统WAPCMS的开发（```这部分是大家都会写的```）。作为核心程序员，我不但完成了网站界面、调度队列的开发工作，更提出了高效的组件级缓存系统，通过碎片化缓冲有效的提升了系统的渲染效率。（```这部分是很多同学忘掉的，要写出你在这个项目中具体负责的部分，以及你贡献出来的价值。```）在该系统上线后，Web前端性能从10QPS提升到200QPS，服务器由10台减少到3台（``` 通过量化的数字来增强可信度 ```）。2008年我升任WAPCMS项目负责人，带领一个3人小组支持着每天超过2亿的PV（``` 这就是Benefit。你能带给前雇主的价值，也就是你能带给新雇主的价值。 ```）。

有同学问，如果我在项目里边没有那么显赫的成绩可以说怎么办？讲不出成绩时，就讲你的成长。因为学习能力也是每家公司都看中的东西。你可以写你在这个项目里边遇到了一个什么样的问题，别人怎么解决的，你怎么解决的，你的方案好在什么地方，最终这个方案的效果如何。

具体、量化、有说服力，是技术简历特别需要注重的地方。

（以上内容在写完简历后，对每一段进行评估，完成后再删除）

---


# 联系方式
（HR会打印你的简历，用于在面试的时候联系，所以联系方式放到最上边会比较方便）

- 手机：15868759135 
- Email：986494553@qq.com 
- QQ/微信号：986494553

---

# 个人信息

 - 朱静迪/男/1998
 - 本科/温州大学城市学院计算机系 
 - 技术博客：http://www.zhanshengpipidi.cn/blog/ ( ``` 使用GitHub Host的Big较高 ```  )
 - Github：https://github.com/xiantang?tab=repositories ( ``` 有原创repo的Github帐号会极大的提升你的个人品牌 ```  )

 - 期望职位：Python爬虫实习生，java后端实习生
 - 期望薪资：3k~5k
 - 期望城市：上海

---

# 在校外包经历

### 知乎问题爬虫项目
使用scrapy框架编写爬虫爬取知乎2016-2017年 50w 条问题的标题描述和关注数，在这个项目中我遇到的最大问题是如何设计爬虫架构，期间尝试了scrapy 框架中的crawlSpider，但是效果不佳，最后突然想到能够通过修改之前写的用户爬虫的爬取的接口，从用户信息接口转为爬取用户问题接口，再筛选问题的时间，最终解决了问题，历时一周。在这个项目中我最满意的是通过定时爬取免费的ip使用redis数据库维护了一个代理池，代理池分别有100+条http/https代理，解决了被网站封禁的问题，在加上这个模块后，爬虫一次性爬取了剩下的45w条数据，原定在30天内完成的项目只用了7天就采集完了。




# 开源项目和作品

### 个人技术博客
使用django+bootstarp搭建的个人博客，在这个项目中我遇到的最大的问题是对django的view 和 model没有理解，导致项目停滞了一段时间，在对数据库和orm有了一定了解后一步一步看着django的文档完成了项目，在这个项目中我最满意的是博文支持Markdown语法，能够写出图文结合的博客，使读者在阅读博客的时候能够产生较好的阅读体验。这个博客对我意义很大，每当我对某些技术有新的见解的时候，我都会打开我的网站写上一篇文章。

### 狗年抢红包游戏
使用java swing 编写狗年抢红包游戏，在这个项目中我遇到的最大的问题是无法实现多键监听，经过翻阅了很多的相关博客，最后选择在每个方向上都设置一个布尔值，在鼠标按下的时候转换为True，松开后变换为False，分别将跳跃和左右移动写入到两个不同线程中，最后解决了问题。在这个项目中我最满意的是我编写的重力类，修改了队友在跳跃线程内设置下落距离来落地的方式，并且解决了无法从高出下落的BUG，保证狗子能在跳跃后顺利下落，优化了跳跃的思路，对之后的编写产生了至关重要的作用。

### 京东商品爬虫
基于scrapy-redis 使用代理服务器池提供代理进行爬取，在这个项目中最让我疑惑的是scrapy-redis是如何实现多台机子共同爬取，和朋友一起研究源码并探讨后得出，scrapy-redis重写了scrapy的调度和队列，将爬取队列存在了redis中的request中，各台机器都通过连接redis取出request并下载，新产生的request再通过enqueue_request放入爬取队列，我们还发现scrapy-redis去重手段十分有趣，他是调用的scrapy中的request_seen方法，采用哈希散列的方法将参数位置不同的url生成相同的哈希地址。这个项目最让我满意的是在并发开启为10的情况下每天能够有10w+条商品60w+评论入库。 



## 技术文章

- [爬虫遭遇状态码521陷阱 破解js加密cookie](https://blog.csdn.net/qq_27302597/article/details/79411808)



# 技能清单

以下均为我熟练使用的技能

- 数据采集: scrapy/selenium/requests/urllib
- Web开发：Python/Java
- Web框架：Django/Jsp
- 前端框架：Bootstrap/HTML5
- 数据库相关：Sqlserver/Mysql/redis(了解)
- 版本管理、文档和自动化部署工具：git


---

# 致谢
感谢您花时间阅读我的简历，期待能有机会和您共事。
